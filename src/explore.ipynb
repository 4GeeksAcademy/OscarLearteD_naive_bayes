{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# **0. Importación de librerías**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import joblib\n",
                "import numpy as np\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import accuracy_score, classification_report\n",
                "from sklearn.model_selection import GridSearchCV\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# **1. Carga, Limpieza y Vectorización de Texto**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Filas: 891, Columnas: 3\n"
                    ]
                }
            ],
            "source": [
                "# Carga del conjunto de datos\n",
                "url = \"https://raw.githubusercontent.com/4GeeksAcademy/naive-bayes-project-tutorial/main/playstore_reviews.csv\"\n",
                "try:\n",
                "    df = pd.read_csv(url)\n",
                "    print(f\"Filas: {df.shape[0]}, Columnas: {df.shape[1]}\")\n",
                "except Exception as e:\n",
                "    print(f\"Error al cargar el dataset: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# **2. Estudio de Variables y su Contenido**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "X_train_vec shape: (712, 3310)\n"
                    ]
                }
            ],
            "source": [
                "# Elimina la columna 'package_name'\n",
                "df = df.drop(columns=['package_name'])\n",
                "\n",
                "# Elimina espacios y convierte a minúsculas\n",
                "df[\"review\"] = df[\"review\"].str.strip().str.lower()\n",
                "\n",
                "# División de los datos\n",
                "X = df[\"review\"]\n",
                "y = df[\"polarity\"]\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "# Transforma el texto en una matriz de recuento de palabras (CountVectorizer)\n",
                "#'stop_words = \"english\"' se utiliza para eliminar palabras comunes sin significado\n",
                "vec_model = CountVectorizer(stop_words = \"english\")\n",
                "\n",
                "# Entrena el transformador en X_train y lo aplica\n",
                "X_train_vec = vec_model.fit_transform(X_train)\n",
                "X_test_vec = vec_model.transform(X_test)\n",
                "\n",
                "# Convierte a array para luego utilizarlo en NB\n",
                "print(f\"X_train_vec shape: {X_train_vec.shape}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## *2.1. Preparación de datos. Entrenamiento de modelos*"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        " Comparación de los diferentes modelos de Naive Bayes\n",
                        "       Modelo  Accuracy\n",
                        "MultinomialNB    0.8156\n",
                        "   GaussianNB    0.8045\n",
                        "  BernoulliNB    0.7709\n"
                    ]
                }
            ],
            "source": [
                "# Multinomial Naive Bayes (MNB)\n",
                "mnb_model = MultinomialNB()\n",
                "mnb_model.fit(X_train_vec, y_train)\n",
                "mnb_y_pred = mnb_model.predict(X_test_vec)\n",
                "mnb_accuracy = accuracy_score(y_test, mnb_y_pred)\n",
                "\n",
                "# Bernoulli Naive Bayes (BNB)\n",
                "bnb_model = BernoulliNB()\n",
                "bnb_model.fit(X_train_vec, y_train)\n",
                "bnb_y_pred = bnb_model.predict(X_test_vec)\n",
                "bnb_accuracy = accuracy_score(y_test, bnb_y_pred)\n",
                "\n",
                "# Gaussian Naive Bayes (GNB)\n",
                "# Se convierte la matriz dispersa a densa (numpy array) para este modelo.\n",
                "X_train_dense = X_train_vec.toarray()\n",
                "X_test_dense = X_test_vec.toarray()\n",
                "\n",
                "gnb_model = GaussianNB()\n",
                "gnb_model.fit(X_train_dense, y_train)\n",
                "gnb_y_pred = gnb_model.predict(X_test_dense)\n",
                "gnb_accuracy = accuracy_score(y_test, gnb_y_pred)\n",
                "\n",
                "\n",
                "# Resultados comparativos\n",
                "\n",
                "resultados = {\n",
                "    'Modelo': ['MultinomialNB', 'BernoulliNB', 'GaussianNB'],\n",
                "    'Accuracy': [mnb_accuracy, bnb_accuracy, gnb_accuracy]\n",
                "}\n",
                "\n",
                "df_resultados = pd.DataFrame(resultados).sort_values(by='Accuracy', ascending=False).round(4)\n",
                "\n",
                "\n",
                "print(\"\\n Comparación de los diferentes modelos de Naive Bayes\")\n",
                "print(df_resultados.to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# **3. Construcción y Comparación del Naive Bayes**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Mejores Hiperparámetros MNB: {'alpha': 0.5}\n",
                        "Accuracy Final (MNB Optimizado): 0.8268\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "# Optimización del MNB\n",
                "param_grid_mnb = {\n",
                "    'alpha': [0.1, 0.5, 1.0, 1.5, 2.0] \n",
                "}\n",
                "\n",
                "mnb_opt = MultinomialNB()\n",
                "\n",
                "grid_mnb = GridSearchCV(estimator=mnb_opt, \n",
                "                        param_grid=param_grid_mnb, \n",
                "                        scoring='accuracy', \n",
                "                        cv=5, \n",
                "                        n_jobs=-1, \n",
                "                        verbose=1)\n",
                "\n",
                "grid_mnb.fit(X_train_vec, y_train)\n",
                "\n",
                "best_mnb_model = grid_mnb.best_estimator_\n",
                "mnb_final_accuracy = accuracy_score(y_test, best_mnb_model.predict(X_test_vec))\n",
                "\n",
                "print(f\"\\nMejores Hiperparámetros MNB: {grid_mnb.best_params_}\")\n",
                "print(f\"Accuracy Final (MNB Optimizado): {mnb_final_accuracy:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## *3.1. Guardado del modelo*"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "✅ Modelo MNB Optimizado guardado como 'multinomial_naive_bayes_optimizado.pkl'\n"
                    ]
                }
            ],
            "source": [
                "joblib.dump(best_mnb_model, \"multinomial_naive_bayes_optimizado.pkl\")\n",
                "print(\"\\n✅ Modelo MNB Optimizado guardado como 'multinomial_naive_bayes_optimizado.pkl'\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# **4. Exploración de alternativas**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## *4.1. Random Forest*"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Accuracy Final (Random Forest): 0.7318\n"
                    ]
                }
            ],
            "source": [
                "rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
                "\n",
                "# Random Forest puede manejar la matriz dispersa, pero es una buena práctica pasarla a densa ya que la dispersa genera errores.\n",
                "X_train_dense = X_train_vec.toarray()\n",
                "X_test_dense = X_test_vec.toarray()\n",
                "\n",
                "rf_model.fit(X_train_dense, y_train)\n",
                "rf_y_pred = rf_model.predict(X_test_dense)\n",
                "rf_accuracy = accuracy_score(y_test, rf_y_pred)\n",
                "\n",
                "print(f\"Accuracy Final (Random Forest): {rf_accuracy:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## *4.2. Regresión Logística*"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Accuracy Final (Regresión Logística): 0.8268\n"
                    ]
                }
            ],
            "source": [
                "log_reg_model = LogisticRegression(solver='liblinear', C=10, random_state=42)\n",
                "\n",
                "log_reg_model.fit(X_train_vec, y_train) \n",
                "\n",
                "log_reg_y_pred = log_reg_model.predict(X_test_vec)\n",
                "log_reg_accuracy = accuracy_score(y_test, log_reg_y_pred)\n",
                "\n",
                "print(f\"Accuracy Final (Regresión Logística): {log_reg_accuracy:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# **5. Comparativa de Modelos**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        " Rendimiento de Clasificadores\n",
                        "                    Modelo  Accuracy\n",
                        "MultinomialNB (Optimizado)    0.8268\n",
                        "       Regresión Logística    0.8268\n",
                        "             Random Forest    0.7318\n"
                    ]
                }
            ],
            "source": [
                "df_final_data = {\n",
                "    'Modelo': ['MultinomialNB (Optimizado)', 'Regresión Logística', 'Random Forest'],\n",
                "    'Accuracy': [accuracy_score(y_test, best_mnb_model.predict(X_test_vec)), \n",
                "                 log_reg_accuracy, \n",
                "                 0.7318] # Valor previamente calculado\n",
                "}\n",
                "\n",
                "df_comparacion = pd.DataFrame(df_final_data).sort_values(by='Accuracy', ascending=False).round(4)\n",
                "\n",
                "print(\"\\n Rendimiento de Clasificadores\")\n",
                "print(df_comparacion.to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# **6. Conclusiones Finales: Mejor Modelo**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "El mejor modelo para esta tarea es el **Multinomial Naive Bayes (MNB) Optimizado**, que alcanzó un `Accuracy` del 0.8268. Este resultado es la validación perfecta de la teoría, que postula que el MNB es el clasificador ideal para este tipo de datos de recuento de palabras. Además del resultado, el MNB suele ser la decisión acertada ya que tiene una mayor simplicidad y velocidad de entrenamiento en PROD.\n",
                "\n",
                "Alternativa Válidas: \n",
                "- Se propuso la **Regresión Logística** como la alternativa más fuerte para superar al Naive Bayes.\n",
                "  - *Resultado*: La Regresión Logística igualó el rendimiento del MNB (0.8268).\n",
                "  - *Argumento*: Los modelos lineales, son extremadamente efectivos en clasificación de texto debido a su capacidad para aprender las ponderaciones de forma aislada en espacios de alta dimensión"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.4"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
